{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned how machine learning allows us to classify data as apples or bananas with a single neuron. However, some of those details are pretty fiddly! Fortunately, Julia has a powerful package that does much of the heavy lifting for us, called [`Flux.jl`](https://fluxml.github.io/).\n",
    "\n",
    "*Using `Flux` will make classifying data and images much easier!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Flux.jl`\n",
    "\n",
    "In the next notebook, we are going to see how Flux allows us to redo the calculations from the previous notebook in a simpler way. We can get started with `Flux.jl` via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful built-in functions\n",
    "\n",
    "When working we'll `Flux`, we'll make use of built-in functionality that we've had to create for ourselves in previous notebooks.\n",
    "\n",
    "For example, the sigmoid function, σ, that we have been using already lives within `Flux`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\u001b[36mσ\u001b[39m\" can be typed by \u001b[36m\\sigma<tab>\u001b[39m\n",
      "\n",
      "search: \u001b[1mσ\u001b[22m log\u001b[1mσ\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "σ(x) = 1 / (1 + exp(-x))\n",
       "```\n",
       "\n",
       "Classic [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function.\n"
      ],
      "text/plain": [
       "```\n",
       "σ(x) = 1 / (1 + exp(-x))\n",
       "```\n",
       "\n",
       "Classic [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function.\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?σ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, `Flux` allows us to *automatically create neurons* with the **`Dense`** function. For example, in the last notebook, we were looking at a neuron with 2 inputs and 1 output:\n",
    " \n",
    " <img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    " We could create a neuron with two inputs and one output via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(2, 1, NNlib.σ)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `model` object comes with places to store weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 1×2 Array{Float64,2}:\n",
       " -1.27546  -0.126819"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 1-element Array{Float64,1}:\n",
       " 0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackedArray{…,Array{Float64,2}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(model.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous notebooks, note that `W` is no longer a `Vector` (1D `Array`) and `b` is no longer a number! Both are now stored in so-called `TrackedArray`s and `W` is effectively being treated as a matrix with a single row. We'll see why in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpful built-in functionality includes ways to automatically calculate gradients and also the cost function that we've used in previous notebooks - \n",
    "\n",
    "$$L(w, b) = \\sum_i \\left[y_i - f(x_i, w, b) \\right]^2$$\n",
    "\n",
    "This is the \"mean square error\" function, which in `Flux` is named **`Flux.mse`**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
