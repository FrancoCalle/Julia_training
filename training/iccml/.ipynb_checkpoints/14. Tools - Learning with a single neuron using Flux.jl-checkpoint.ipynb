{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with a single neuron using Flux.jl\n",
    "\n",
    "In this notebook, we'll use `Flux` to create a single neuron and teach it to learn, as we did by hand in notebook 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data and process it\n",
    "\n",
    "Let's start by reading in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using TextParse\n",
    "using DataFrames\n",
    "\n",
    "applecols, applecolnames = TextParse.csvread(\"data/Apple_Golden_1.dat\", '\\t')\n",
    "bananacols, bananacolnames = TextParse.csvread(\"data/bananas.dat\", '\\t')\n",
    "\n",
    "apples = DataFrame(Dict(strip(name)=>col for (name, col) in zip(applecolnames, applecols)))\n",
    "bananas = DataFrame(Dict(strip(name)=>col for (name, col) in zip(bananacolnames, bananacols)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and processing it to extract information about the red and green coloring in our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = :red\n",
    "col2 = :green\n",
    "\n",
    "x_apples  = [ [apples[i, col1], apples[i, col2]] for i in 1:size(apples)[1] ]\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "xs = vcat(x_apples, x_bananas)\n",
    "\n",
    "ys = vcat( zeros(size(x_apples)[1]), ones(size(x_bananas)[1]) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is in `xs` and the labels (true classifications as bananas or apples) in `ys`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Flux.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load `Flux` to really get going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the last notebook that σ is a built-in function in `Flux`.\n",
    "\n",
    "Another function that is used a lot in neural networks is called `ReLU`; in Julia, the function is called `relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Use the docs to discover what `ReLU` is all about.\n",
    "\n",
    "`relu.([-3, 3])` returns\n",
    "\n",
    "A) [-3, 3] <br>\n",
    "B) [0, 3] <br>\n",
    "C) [0, 0] <br>\n",
    "D) [3, 3] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a single neuron in Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `Flux` to build our neuron with 2 inputs and 1 output:\n",
    "\n",
    " <img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously put the two weights in a vector, $\\mathbf{w}$. Flux instead puts weights in a $1 \\times 2$ matrix (i.e. a matrix with 1 *row* and 2 *columns*). \n",
    "\n",
    "Previously, to compute the dot product of $\\mathbf{w}$ and $\\mathbf{x}$ we had to use either the `dot` function, or we had to transpose the vector $\\mathbf{w}$:\n",
    "\n",
    "```julia\n",
    "# transpose w\n",
    "b = w' * x\n",
    "# or use dot!\n",
    "b = dot(w, x)\n",
    "```\n",
    "If the weights are instead stored in a $1 \\times 2$ matrix, `W`, then we can simply multiply `W` and `x` together instead!\n",
    "\n",
    "We start off with random values for our parameters and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rand(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the product of `W` and `x` will now be an array (vector) with a single element, rather than a single number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our bias `b` is treated as an array when we're using `Flux`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Write a function `mypredict` that will take a single input, array `x` and use `W`, `b`, and built-in `σ` to generate an output prediction (stored as an array). This function defines our neural network!\n",
    "\n",
    "Hint: This function will look very similar to $f_{\\mathbf{w},\\mathbf{b}}$ from the last notebook but has changed since our data structures to store our parameters have changed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Define a loss function called `loss`.\n",
    "\n",
    "`loss` should take two inputs: a vector storing data, `x`, and a vector storing the correct \"labels\" for that data. `loss` should return the sum of the squares of differences between the predictions and the correct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating gradients using Flux: backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning, we know that what we need is a way to calculate derivatives of the `loss` function with respect to the parameters `W` and `b`. So far, we have been doing that using finite differences. \n",
    "\n",
    "`Flux.jl` instead implements a numerical method called **backpropagation** that calculates gradients (essentially) exactly, in an automatic way, by indirectly applying the rules of calculus.\n",
    "To do so, it provides a new type of object called \"tracked\" arrays. These are arrays that store not only their current value, but also information about gradients, which is used by the backpropagation method.\n",
    "\n",
    "[If you want to understand the maths behind backpropagation, we recommend e.g. [this lecture](https://www.youtube.com/watch?v=i94OvYb6noo).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, `Flux` provides a function `param` to define such objects that will contain the information for a *param*eter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start, as usual, by setting up some random initial values for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_data = rand(1, 2)  \n",
    "b_data = rand(1)\n",
    "\n",
    "W_data, b_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up `Flux.jl` objects that will contain these values *and* their derivatives, and allow to propagate\n",
    "this information around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = param(W_data)\n",
    "b = param(b_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `param` is a function that `Flux` provides to create an object that represents a parameter of a machine learning model, i.e. an object which has both a value and derivative information, and such that other objects know how to *keep track* of when it is used in an expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "What type does `W` have?\n",
    "\n",
    "A) Array (1D) <br>\n",
    "B) Array (2D) <br>\n",
    "C) TrackedArray (1D) <br>\n",
    "D) TrackedArray (2D) <br>\n",
    "E) Parameter (1D) <br>\n",
    "F) Parameter (2D) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "`W` stores not only its current value, but also has space to store gradient information. You can access the values and gradient of the weights as follows:\n",
    "\n",
    "```julia\n",
    "W.data\n",
    "W.grad\n",
    "```\n",
    "\n",
    "At this point, are the values of the weights or the gradient of the weights larger?\n",
    "\n",
    "A) the values of the weights <br>\n",
    "B) the gradient of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "For data `x` and `y` where\n",
    "\n",
    "```julia\n",
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "```\n",
    "apply the loss function to `x` and `y` to give a new variable `l`. What is the type of `l`? (How many dimensions does it have?)\n",
    "\n",
    "A) Array (0D) <br>\n",
    "B) Array (1D) <br>\n",
    "C) TrackedArray (0D) <br>\n",
    "D) TrackedArray (1D)<br> \n",
    "E) Float64<br>\n",
    "F) Int64<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these features to reimplement stochastic gradient descent, following the method we used in the previous notebook, but now using backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Modify the code from the previous notebook for stochastic gradient descent to use Flux instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the values stored in `b` before we run stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `stochastic_gradient_descent`, we find the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_final, b_final = stochastic_gradient_descent(loss, W, b, xs, ys, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can look at the values of `W_final` and `b_final`, which our machine learned to generate our desired classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Plot the data and the learned function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "\n",
    "Do this plot every so often as the learning process is proceeding in order to have an animation of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation with Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to repeat the above process for a lot of different systems.\n",
    "Fortunately, `Flux.jl` provides us with tools to automate this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux allows to create a neuron in a simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `2` and `1` refer to the number of inputs and outputs, and the neuron is defined using the $\\sigma$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made an object of type `Dense`, defined by `Flux`, with the name `model`. This represents a \"dense neural network layer\" (see later for more on neural network layers).\n",
    "The parameters that will be modified during the learning process live *inside* the `model` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "\n",
    "Investigate which variables live inside the `model` object and what type they are. How does that compare to the call to create the `Dense` object that we started with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model object as a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the `model` object to data just as if it were a standard function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(rand(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    "\n",
    "Prove to yourself that you understand what is going on when we call `model`. Create two arrays `W` and `b` with the same elements as `model.W` and `model.b`. Use `W` and `b` to generate the same answer that you get when we call `model([.5, .5])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to provide Flux with three pieces of information: \n",
    "\n",
    "1. A loss function\n",
    "2. Some training data\n",
    "3. An optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux has various loss functions built in, for example the mean-squared error (`mse`) that we have been using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.mse(model(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common one is the cross entropy, `Flux.crossentropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can take a couple of different forms. \n",
    "One form is a single **iterator**, consisting of pairs $(x, y)$ of data and labels.\n",
    "We can achieve this with `zip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "\n",
    "Use `zip` to \"zip together\" `xs` and `ys`. Then use the `collect` function to check what `zip` actually does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to tell Flux what kind of optimization routine to use. It has several built in; the standard stochastic gradient descent algorithm that we have been using is called `SGD`. We must pass it two things: a list of parameter objects which will be modified by the optimization routine, and a step size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD([model.W, model.b], 0.01)\n",
    "# give a list of the parameters that will be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient calculations and parameter updates will be carried out by this optimizer function; we do not see those details, but if you are curious, you can, of course, look at the `Flux.jl` source code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces in place to actually **train** our model (a single neuron) on the data. \n",
    "\"Training\" refers to using pre-labeled data to learn the function that relates the input data to the desired output data given by the labels.\n",
    "\n",
    "`Flux` provides the function `train!`, which performs a single pass through the data and does a single step of optimization using the partial cost function for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then just repeat this several times to train the network more and coax it towards the minimum of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    Flux.train!(loss, data, opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the parameters after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing out a list of parameters to modify, `Flux` provides the function `params`, which extracts all available parameters from a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(params(model), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "\n",
    "So far we have just used two features, red and green. \n",
    "\n",
    "(i) Add a third feature, blue. Plot the new data.\n",
    "\n",
    "(ii) Train a neuron with 3 inputs and 1 output on the data.\n",
    "\n",
    "(iii) Can you find a good way to visualize the result?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
